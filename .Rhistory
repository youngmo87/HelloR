nouns = sapply(tdf, extractNoun, USE.NAMES = F)
# 1. 명사 추출
nouns = sapply(tdf, extractNoun, USE.NAMES = F)
setup_twitter_oauth(api_key, api_secret, token, token_secret)
tweets = searchTwitter(enc2utf8('공무원'), n=200, lan='ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # EMO(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
nouns = sapply(tdf, extractNoun, USE.NAMES = F)
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
names(wc1)
length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
wc2
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('공무원'), n=300, since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # EMO(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
#nouns = sapply(tdf, extractNoun, USE.NAMES = F)
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('공무원'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # EMO(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
#nouns = sapply(tdf, extractNoun, USE.NAMES = F)
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
sys.getlocale()
guess_encoding(tdf)
Encoding(tdf)
tdf = twListToDF(tweets)
guess_encoding(tdf)
source('C:/workspace/r/hellor/data/libraryinit.R', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('공무원'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # EMO(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('공무원'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # EMO(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
tw = enc2native(tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('공무원'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
tw = enc2native(tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
unlink('0409exam_cache', recursive = TRUE)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('딥러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
tw = enc2native(tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
unlink('0409exam_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('딥러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 200)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
View(tdf)
View(tw)
View(wc2)
api_info[1]
api_info[2]
guess_encoding(wc2)
enc2utf8(wc2)
enc2utf8(tw)
unlink('20190409exam_cache', recursive = TRUE)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('승리'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = enc2utf8(tw)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
Sys.setlocale("LC_ALL", "korean")
Sys.setlocale("LC_ALL", "korean")
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
닝
Sys.setlocale("LC_ALL", "korean")
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('머신러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
tw = enc2utf8(tw)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('딥러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
#tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
#tw = enc2utf8(tw)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('딥러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
#tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
#tw = enc2utf8(tw)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
#tw = gsub('\\p{So}|\\p{Cn}', '', tw, perl = TRUE)    # EMO(/U00000f등) 제거 (mac)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
unlink('20190409exam_cache', recursive = TRUE)
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_111/")
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_111/")
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_191/")
library(rJava)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('딥러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
#tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
#tw = enc2utf8(tw)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
install.packages(c("rJava", "memoise", "KoNLP"))
install.packages(c("rJava", "memoise", "KoNLP"))
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_191/")
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_111/")
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_191/")
library(rJava)
library(KoNLP)
library(twitteR)
library(RCurl)
library(RJSONIO)
library(stringr)
library(streamR)
library(ROAuth)
library(RColorBrewer)
library(wordcloud)
load('data/api_info.rda')
setup_twitter_oauth(api_info[1], api_info[2], api_info[3], api_info[4])
tweets = searchTwitter(enc2utf8('딥러닝'), n=300, lan = 'ko', since = '2019-03-11', until = '2019-03-31')
tdf = twListToDF(tweets)
#tdf = tdf %>% filter(!isRetweet) %>% filter(favoriteCount > 2)
tdf = tdf %>% filter(regexpr('광고',text) == -1)    # 특정 단어 포함된 문서 제거
tw = unique(tdf$text)
#tw = enc2utf8(tw)
tw = gsub("[[:cntrl:]]", "", tw)
tw = gsub("http[s]?://[[:alnum:].\\/]+", "", tw)
tw = gsub("&[[:alnum:]]+;", "", tw)
tw = gsub("@[[:alnum:]]+[:]?", "", tw)
tw = gsub("[ㄱ-ㅎㅏ-ㅣ]","",tw)                   # 한글 불용어(ㅋㅋㅎㅎ ㅠㅜ등) 제거
tw = gsub("<.*>", "", enc2native(tw))          # emoji(/U00000f등) 제거 (windows)
tw = gsub("\\W", " ", tw)
tw = gsub("\\s{2,}", " ", tw)                  # 2개이상 공백을 한개의 공백으로 처리
tw = gsub("[[:punct:]]", "", tw)
# 1. 명사 추출
wc = sapply(tw, extractNoun, USE.NAMES = F)
wc1 = table(unlist(wc))
#names(wc1)
#length(wc1)
wc2 = head(sort(wc1, decreasing = T), 100)
pal = brewer.pal(9, "Set1")
wordcloud(names(wc2), freq=wc2, scale=c(5,0.5), rot.per=0.015,
min.freq = 1, random.order = F, random.color = T, colors = pal)
